30/11/19

Currently working on trying to get more types of files reading in correctly. 

Stuck on my little endian test file and its highlighted the need to implement some proper error handling messages.

The first error message I need to implement is when the requested channel name doesn't exist.

Actually, before I do that it appears I can't parse TestFile_LE.tdms correctly at all.
It's incorrectly parsing the total size of the data nominally in the segment.
Checked it in hxd, and my parser is correctly reading the 8 bytes (u64) value in the file.
The file doesn't make sense and I wonder if it's malformed? Except that the excel plugin appears to be able to parse the file just fine.

Figured it! The spec states that the total size value is only stored for variable length data types. Of the tdms data types I think only strings are variable length types.

The various optional parts of the properties are wrapped in options during initialisation and initialised depending ont eh value of the raw data index. I'm not sure this is the most graceful way, will have a quick play with an alternative structure.

Ok, next error, after reading the meta data the chunk size field has been set to zero. This causes a later part of the program to fall down as it tries to divide by chunk size. So either it's mistakenly computed zero or I need to handle zero sized chunks.

1/12/19

chunk_size is meant to be a helper to do something but I can't remember exactly what (bad commenting.) It appears to be an accumulating track of object size, except that I had mistakenly linked it directly to total_size, which is actually just to do with string values. I think I need to make a choice on how to compute chunk size depending on data type.

Using Adam's nptdms as reference confirms my idea, need to calculate chunk size as no_values * data_type_size * dim. I don't have any information encoding datatype size at the moment.

I think I don't want to carry that information around with the datatype so the best place to do it is in the object read in method.

I created a size function as an implementation on DataTypeRaw. It returns an error if called with DataTypeRaw::TdmsString as string has no defined size. A guard clause is required in the calling function as a file read is required to determine string size. This size function is intended solely for use in computing total chunk size. It would be great if it could be a const function.

Attempting to run and it got a lot further, choked trying to unwrap an option that was None, not sure where that came from. There are 3 possible call sites.
Fixed the first call site I looked at which was reading in a datatype for a property. I implemented the same kind of error as when reading an object datatype in.

2nd call site looks like it might be the nuts. It's unwrapping a path which I believe is the one I've provided to try and tell it to load a channel. This will fail hard if channel is wrong. That being said I commented out trying to load a specific channel so perhaps it's not this but it will need to be fixed.

3rd call site is where I attempt to copy across from a previous object if raw_data_index == 0. This is another stinker. It's actually quite a complex look back with a number of possible ways to fail. I'm having trouble thinking of all the right methods on option/result to use to keep the chain going.

There could be no previous segment, then the previous segment could have no meta data, then that meta data could fail to contain an object addresable by the current objects path (in other words could fail to contain a prior instance of the object)

Have tried a couple of variants, one looks like it will compile but is ugly, one looks nice but I'm struggling with the lifetimes. Nap time.

2/12/2019

I fixed the object read code so it returns more meaningful errors which confirmed that somehow I'm getting NoPreviousObject error with a raw_index == 0. So I'm thinking maybe the path has a problem. The paths in the log look like:
/'Group2'/'I32WavRamp'
The slash characters should be part of the string? Perhaps that's getting messed up somehow.

Also, I'm beginning to think maybe the objects map should be promoted to a first order representation of the data rather than buried in the meta data struct which is a more file structure oriented represenation rather than data oriented.

So it's definitely the case that this part of the file is packaged as a raw_data_index == 0 section, so the problem is that we can't find the object in the look up.

Actually, I think it's legitimately the case that the previous segment doesn't have that channel in it. I don't see that the segments can be out of order so that implies it's possible for the relationship to look back further than the immediately prior segment.

So if it can go back further then I could either promote objects to something like a simple list, but then I have to know how many segments I'm in and where those segments are in memory. This would speed up reads to the objects after they've been mapped, as opposed to what I'm currently doing which is re-walking the entire segment list looking for the object.

Already kind of have this stored at the file level, just need to mod it store a vector of segment indexes rather than a meaningless number.

3/12/19

Ok, getting back into the load_data function as it's currently squatting on some of the functionality that should probably be in map_segments.

Currently we iterate over segments and dive into the meta_data to check if the object is in there. If it is there, then extract all the information about it.

While within that segment construct a set of read pairs by iterating over the no_chunks in that segment. Mental block here.

I've massively misunderstood the concept of raw_data_index, it's nothing to do with where raw data lives. It's the length of the index information. It's always either 20 or 28 depending on whether it's "defined" data or "variable length" i.e. Strings.

Raw data arrays for all channels are concatenated in the exact order in which the channels appear in the metadata for the segment.

I'm still confused by whether there can be more than one chunk ("multiple channels") per segment, I think there can be in which case it's a repeating pattern of reads to get at a single channel, iterate over chunks jump straight to index within chunk based on knowing previous accumulated channel sizes (effectively index of channel raw data within chunk)

5/12/2019

Heavily modified the way the read segment, meta data and object code works to simplify storage of mapping information required for late reads. At least I hope it's simplified. The goal is once the object list bubbles back up to the segment level we take that opportunity to build a map for that object in that segment. The map consists of a vector of structs which encapsulate an absolute index and a number of bytes to read (though - this may have to be no_values not no_bytes given the use of read_u32).

On an object by object basis if there's raw data we build this map then check if that object is in the global object map. If it is we add the extra map to any existing mapping. If it's not then we insert it.

Then loading data becomes as simple as checking if the object is in the map then iterating over the read pairs and building a single contiguous vector.

7/12/2019:

Given that we're trying to look back to the last instance of the object we have to store the actual last instance of the object observed.

Shit, read completed.

I implemented storage for the last instance of an object any time the file level object map is hit.

The file was very slow, I wasn't sure if it's because I'm cloning the objects outright or whether it's because I'm printing so much crap to console and log files. It generated a 13 MB log file. Disabling logging at the debug level massively sped up the run.

Ok, I'm not getting any data out of the data load operation. Putting in an obviously bad channel name correctly returns an error that the channel isn't found. Obviously that's because I haven't implemented vector reads for all datatypes.

12/12/2019
Updates spotty sorry, have been trying to debug what was going on. I implemented a read type for double and tested it. Had garbage data that ended up being because I was confusing # bytes with # of values. In any case, it now works for contiguous double data. But during debugging I remembered I have to implement interleaved support as well if I want to load other data channels from my test file.

To build a read map for interleaved data you have to know how many objects are in the segment. It would be super inefficient to seek and load alternating values I think so we'd potentially want to block load all the raw data in the segment. Perhaps Bufrdr just handles this though?

The dumbest way I could extend my existing abstraction of read pairs would be to set up a pair for every interleaved data point, I'm just gonna try it and see how stupid it is

4/10/2020

Long time since I picked up the project. I'm pausing the interleaved data read in the hopes of getting an "easy win". I want to implement some plotting code using one of the plotting libraries to display the read in data in a more rewarding form.

Initial attempt with the "charts" library.

Woohoo, got it to work with my dummy file which clearly shows why I needed to get interleaved working. You see a saw tooth of values from 0 to 1 (the dataset is a simple ramp of doubles going from 0 to 1)

Tried to get a signal express test file to load and hit the chunk_size = 0 problem (div by zero)

7/10/20

Ok, starting again with the signal express file logs to try and ID why I got chunk size 0.

Working my way back up through some of the decisions I made previously, I've gotten rid of maintaining the interleaved flag in the TdmsFile struct as the spec strictly states that it can be varied segment to segment.

For similar logic I should probably not maintain the endianess flag next to the file handle, but I'm not sure how to configure the read methods gracefully if I don't.

Have hoisted the endianess as an input variable to the read operations, it ends up looking a lot nicer. It means the caller functions in the read segment use their local endianess information to say how they want to read.

I've also commented out the datavector read because it's a bit of a shit fest at the moment and the endianess handling change breaks it in fairly fundamental ways.

Have implemented the builder pattern (I think?) for TdmsSegment, same as TdmsMetadata in the interest of separating out some of the "massive functionness" of it, bits of segment initialisation are more self contained now, i.e. read lead in.

8/10/2020

Ok, got it to recompile after I got the function signatures tidied up, had to change the read helpers to use a reference to endianness, rather than trying to take ownership of it. Back to the divide by zero error.

Implemented builder pattern for Property but need to do so for object.

Have implemented a guard clause that checks if chunk size is non-zero before trying to compute with it. If chunk_size is zero, then there's no raw data for the object in the segment. Have to think throug whether this follows more directly from just checking the ToCMask

The guard clause got the signal express file to load but the layout looks like a total abuse of the spec, stuff is scattered all over segments. Also some of these segments ToCMask says they contain raw data but they don't appear to atually have any.

I think this might be key from the spec:
"Writing a new object to the next segment will imply that the segment contains all objects from the previous segment, plus the new objects described here. If the new segment does not contain any channel(s) from the previous segment, or if the order of channels in segment changes, the new segment needs to contain a new list of all objects. Refer to the Optimization section of this article for more information."

10/10/2020
Diving into the signal express loading approach. The key here is that unless you get a full object list then any new objects are assumed to be new, and I guess concatenated at the end?

So how do you know when you receive an object list that it's a full new one or not? Maybe it's if you get a list of objects but the new objects tab is false?

Also another thing to figure, objects can keep accumulating properties, reviewing if I've captured that objects are really first class things that you need to keep a track of and grow?

Not really, objects are currently contained within metadata, that's kind of the right way to think of it, each segment has "MetaData" but then each object also has meta-data. I think I can "promote" the TdmsMetaData struct into being part of the segment

There may be a subtle bug here, currently if an object has the same raw data indices, I copy it to a new object and push that object into the object map, but this likely results in duplicate reads, as I think what it actually means is "I haven't written any new raw data so all previous indices are valid"

I need to refactor the read object method to update an object in the object map, rather than blindly read and treat it as if it's a totally new object.

Have pushed down the object map stuff to the read object method but haven't refactored yet

11/10/2020
Struggled to get anywhere

12/10/2020
If we can get an existing object from the hashmap, then clone it, if not initialize it.

Added a place to handle the DAQmx data but no handling yet.

It actually doesn't work to compute read pairs at the read object level, you need meta data from the segment like its current data offset. What I've done instead is as per first sentence, clone the object then push new properties to it. It's a little inefficient but it then returns that object, then the segment parser goes over the objects and will update the object map with the latest copy. It's double handling but hopefully low cost (if there were a lot of properties it could be inefficient). Slightly more efficient would be to update in place and set a flag then update the read pair information and clear the flag.

Ok, it's pulling and printing properties, not all that excel picks up but I think they're weirdly split across objects with similar names.

It appears to be a weird way signal express does thing, some of the properties are stored against the group but then excel they're plotted as if they're by channel. Decimation levels are groups (why they fuck is this a file encoded thing?)

11/11/2020

Attempt to pick back up.

I thought it might help me resolve some of my questions about what format to return data in to actually have a customer, i.e. the interface with python I originally envisioned.

Ok, directly implementing against the numpy spec looks enormously complicated.

26/05/2021

Dusting back off, I've forgotten where I got to with Signal Express weirdness, so I'm just going to attack a standard labview and brute force the loading

27/05/2021

I've just noticed that byteorder implements write_f32_into function which can handle multiple points at the same time, though the dest is a mutable reference to an array, so might be a bit more awkward. And you have to read the full amount into the buffer otherwise you hit an error.

Abandoning that after some discussion on forums. Will just use the preallocated vec approach.

Also from forums someone agreed that it's technically possible for endianess to change segment to segment so I should handle it for robustness sake. I wonder if nptdms does?

Refreshing my memory of the file spec, particularly the optimization section:
1) - Writing a new object to the next segment will imply that the segment contains all objects from the previous segment, plus the new objects described here. If the new segment does not contain any channel(s) from the previous segment, or if the order of channels in segment changes, the new segment needs to contain a new list of all objects. Refer to the Optimization section of this article for more information.
2) - Writing a new property to an object that already exists in the previous segment will add this property to the object.
3) - Writing a property that already exists on an object will overwrite the previous value of that property.

QUESTION/TODO:
1) Am I handling over-writing properties correctly?
2) Am I handling adding properties correctly?
3) What flag is set to let you know you have a new object list, rather than the implied "the objects are still present"
---"You must set the ToC bit kTocNewObjList" when a channel drops out.


DONE:
Why do I have a FileHandle struct, just for the ability to implement against it? Is that worth having to write "handle.handle" when I want to do low level read without check?
- Not going to bother tinkering with this just yet

TODO:
I don't understand under what circumstances kToCMetaData would be false as the optimisation section of the spec implies that if there are no object changes, then the raw data simply gets appended and the lead in updated.

DONE:
I'm starting to think the object map ancillary structure is the right way to go given the segment centric view has all sorts of things going on like implicit object presence unless kTocNewObjList bit flag is set.
Do I need a properties map? I think maybe not as all properties are implicitly encapsulated within objects.

TODO:
If I accept that endianess can change between segments, and that an object can straddle segments, then the ReadPair struct should probably carry endianess with it. Is there 1 ReadPair per segment?
-> No, depends on # of chunks in the segment, I guess if multiple writes have occurred without updating object info?
-> So there needs to be a struct that wraps the Vec<ReadPair> with endianess for that segment

TODO:
Related to the above, I need to take a step back and think about where the raw information should flow to. I don't need to hang on to every bit flag for example, I only need it transiently while I figure out the changes in objects, properties and raw data.

28/05/2021
Working on a visual representation of some of the optimisation stuff that was confusing me

30/5/21
Object order is important! Particularly with implicit object presence

Conceptual, what if we read a new section, ToCNewObject not set so must implciitly have previous objects, but we read a property from an object mid way through the list.

Update or add property to that object, then track through all other objects in order, so must store
order somehow. Possibly replace relative position? Objects are currently a vec in the "file centric" data structure, so implicitly ordered there, but then I put it into a hashmap. In npTDMS adam constructs that look up dict on every segment read from the list of ordered objects and keys it by the objects path.

So store in order by default, but update a hashmap on each read, man that's basically what I've got!

25/8/2021

Here we go again on the pick up :(.

Trying to get refamiliar with the read_metadata function which is where the above is happening

Ok, thoughts as I look at continuing the implementation of the object search and update code:
1) I do bunch of work pulling object_maps from all_objects (destructively via remove), then update those object_maps, then insert them into a new map, then extend the new map with the remainder of the old map.

-> The above is wrong ordering, I need to do the read_pair computation on the new map before extending it, then use that to reset live_objects map, then finally extend it and use it to reset the all_objects map.

Ok, end of session I think I've handled the case where a new object list is flagged and everything needs to be updated, next is the "else" case where it's not a new object list and we have to use live_objects map + the new objects to work out all the data in the segment.

26/8/2021

Continuing as above.

Notes:
1) I'm still not convinced that I actually need live_objects. It depends on whether the prior segment reliably contains all objects. As I think about this though, there is an obvious scenario where you have multiple segments each with one additional new object. As I only store new objects in the vectors maintained in the segment data structs you wouldn't realise you're trackin an accumulating list. So live_objects definitely necessary.
2) I don't like the cloning I'm having to do, actually, I just realised I don't need to, the only reason I was cloning and pushing the last_objects into a vec was to then iterate over them accumulating size so I can do the read pair calculations, but I can actually just do this as I go again.
3) Can an object that is implicitly being tracked change size? Yes, a channel anywhere in the stack can change it's size, which will result in the new raw data index being recorded in the metadata for the segment under that channel name, but otherwise no change. So we actually have to check whether any new things are old things in our live_object and update as necessary.
4) I'm trying to figure out how to update the read_maps efficiently, I'm not sure it's possible so just going to double iterate, once over the new objects, then over  the live_objects (with new index info) to append new read_map points. The problem is the lack of info about the chunk_size. As far as I can see, I can't obtain the chunk size without walking the updated live_objects list and I can't compute all the read pairs without the chunk_size

End of session: I finished implementing the update live_objects arm. I've also tidied up the _read function to remove the prior way of doing things.

Things to do:
1) Handle all the compilation errors - DONE
2) Still only handling non-interleaved data

27/8/2021
Have just finished dealing with some compilation errors related to ownership. I was trying to iterate over values in the all_objects map to push them into the new_map. I fixed it by draining the old all_objects map, but I think I could just extend the old map with the new one? Because I removed each object to modify it, the two maps are gauranteed to be mutually exclusive.

Yes-> that builds without issue.

28/8/2021

Tidying up some of the warnings. Next steps are to advance a gui implementation (to give a customer for the library) and to implement more data vectors.

Have the template UI running from within my main. Next step, put a plot widget in there.

A lot of the code will have to move across to app.rs, main.rs in the egui thing is a simple launcher for the main gui thread.

Side note:
1) Need to move away from requiring people to manualyl call map_segments prior to calling any other function. Basically it should probably map on load. - DONE

Implemented map on load, made map_segments private

29/8/2021
[11:22 PM] konsumlamm: why exactly don't you just make a generic type for this (possibly with some trait constraint)?
struct DataTypeVec<T>(Vec<T>);

Got a double vector type plotted int the front end (test file: TestFile_LE.tdms) but I noticed it was hideously slow to load all the objects for display. Not sure if that's because it's printing or logging a bunch of stuff?

Dumbly attempting a release build to see what happens
-> noticeably faster but still slow.

Also, need a scroll in that side panel (DONE)

Tried loading the big file in release mode, it is horrifically slow, took minutes to parse through
to the objects list. It did right a 600 MB log file though, so what if I turn that off or at least limit it to error. Yeah, that did it, down to seconds to parse the file (probably about 5-6ish?). The graph is sluggish though, not sure if it's because it's doing the load every time but that's most likely.

It takes >20 s to open in excel then you're prompted to load due to a value limitation, so not bad.

Will try caching the data, an interesting problem because how do I make a generic cache?

I guess just cache the DataTypeVec enum for starters (I'm at risk of just spinning my wheels optimizing a toy app here.) Actually, why not just cache the line? Can't cache the Line because it's not copy and the ui.add method takes ownership of it.

All I could eliminate was the load so it's only slightly faster. Tried to cache Values but that doesn't implement clone either.

Removed caching, the creator of egui said painting dense lines is slow and they should be downsampled prior to painting to make them easier to handle. That's a future problem.

End of day:
Stuck on trying to make the datavector generic. The only way is for the datavectors or its constituents (?) to all conform to a trait contract and then for generic functions to work with them. Feels like going back in circles to thoughts I  was having a long time ago.

What if I were to provide "as xx" implementations for standard types? Maybe a large matrix of possibility unless From/Into is clever (is it even the right thing?)

30/8/2021
Doing some research on how to make things generic. I think I could use something like the num-traits crate to provide a mechanism for at least making all the numeric raw types usable in the same way. But I'm not sure if that would really help the end user unless they also happened to be using num-traits. They do provide big ints and complex numbers, and they have a huge install base so probably a lot of interop.

Ahahaha, just checked my cargo.tomly and it's already there, not sure what I was doing with it though. Ahh, I was using it for FromPrimitive, to convert the primitive into a value of an enum, applied by #deriving it for DataTypeRaw. Adam uses num_enum::TryFromPrimitive in a similar way

ndarray_npy seems like it has some clever ideas for apparent genericism as well (https://docs.rs/ndarray-npy/0.8.0/ndarray_npy/index.html) but it does have a really obvious end point that it's targetting

10/9/2021

The generic struct didn't really feel like it was going anywhere. Looking instead at some simple things like incorporating adams generic over endianess approach. Also looking at perhaps refactoring a bit away from one monolith

Tried to use "read_into" to read multiple bytes in one go rather than iterating the for loop, but Vec::with_capacity doesn't actually create a slice that can be indexed, the memory is uninitialized.

11/9/2021

Doing some reading implied that initializing the memory i.e. vec![0; N] is likely slower, but maybe it's a non-issue? I assume that the same rules apply to arrays or alt that arrays are always initialized to something. Yes, teh second, MaybeUninit is required to allow arrays to be instantiated uninitialized.

Got the read_into version working, it feels a bit faster in gui land, which I hope is because my stupid load from file every time approach is acting as a stress test lol.

+ Cleared the chart stuff out of main
+ Cleared a bunch of warnings related to unused imports, unused variables etc 
+ Addressed a bunch of clippy lints (manual assign, pointless dereference)

Committed

Starting work on generic reader functions. Notes as I go:
- Back to some file.handle.handle stuff which just looks terrible. I'm going to get rid of FileHandle

+ Deleted FileHandle and moved it's impls to TdmsFile

- At the load_data level (public api) in the current architecture I need to be able to know what the endianess so I can call read_data_vector with endianess defined
- Following the builder pattern really overcomplicated things I think. Removing un-necessary builder patterns.

+ Got rid of the builder pattern from TdmsMetadata and TdmsObject, end up wondering why they're impls against the struct given they don't take a self parameter.

- Don't appear to be able to store endianness in an enum and then use it as a type annotation. Confirmed not possible, rust doesn't try to infer anything about the variants of an enum. Not sure whether to chop it out completely. Adam split the read immediately after he read the ToC flag and passed it into the function in an if/else with the type annotation (BigEndian) hard coded.
- Just figured out the endianness enum only compiled because it was ignoring me trying to put the byteorder enums into it. It actually totally makes no sense.

+ Replacing endianess with a simple bool

- Next need to kill the builder pattern for segment

12/9/2021

I tried to create a struct:
struct Endianness<T: ByteOrder> {
    e: T,
}

it doesn't work because there's no valid constructor for the Endianness enums provided by byteorder.

Still stuck on read_data_vector, have figured out how to record the endianess (went back to keeping it in objectmap) but now I get ownership issues trying to borrow the object map for key information and also borrow self to read the file handle.

Ok got it back to a compiling point

Copying out all these vector read methods are exactly as shit as I imagined, super easy to fuck up the # of bytes to turn total_bytes into #values

Implemented down to Double in the vectors

Things I'm not currently handling:
- Extended (128)
- Floats with units
- ComplexNumbers
- Daqmx

Apparently NI doesn't even support writing complex numbers to tdms so shrug on that for now.

27/1/2022

Back on the horse, trying to come up with a reasonable API for the library, 2 goals, plug into polars and plug into plotting software I'll write.

I'm converging towards what Adam was doing with a "generic over reader" signature for the functions.

Also towards stripping out the DataType enum or at least its implementation currently. It's really only used to wrap an Object Property value. To be fair to myself
Adam's solution is identical, just better organized (not dependent on being called from within a method acting on a "File" struct)

I also like the way his read stuff lives by the types and is out of the way of the rest of the actual walking the file.

Will start by extracting the file handle stuff from my TdmsFile struct, akin to what Adam has done and feel it out

28/1/2022

Apart half way through ripping it apart and re-factoring. Currently working on trying to re-factor the segment reading code so it's a method on TdmsMap instead of TdmsSegment

Changes so far:
+ Changed TdmsFile -> TdmsMap
+ Removed all methods from TdmsSegment, only remaining thing is a Display implementation
+ Removed the Filehandle from TdmsMap as a maintained element, the reader gets instantiated in the open function but not stored
+ Moved TdmsSegment reading methods into TdmsMap and re-combined read_leadin with read_segment
+ Stopped storing booleans for the ToCMask flags, just storing ToCMask now
+ Instantiate segment in read_segment

It probably actually makes sense to put  the reader thing back in, but I might just be wrestling with the public API too early.

Also hitting issues with the Object read code need to tidy up the responsibilities.

29/1/2022

Continuing from above, tidying up the UI code which has changed API with a version bump

Further changes
+ Updated app.rs to be congruent with the latest version of egui
+ Moved all the helper functions under the new TdmsFile struct which has the TdmsMap struct as a field.
+ Deleted object copying from the lowest level object read function, this should only happen as part of the update index step of
read meta data (this should become its own function.)
+ Now generic over reader everywhere, so it'd be interesting to swap to something other than bufreader at some point

It's still super slow to just get ahold of the channel names.

Also now all the channels are panicking at 'not_implemented'

I think I broke the object map by removing that low level copy op, need to go over the algorithm again to figure out what I did

30/1/2022

Thinking about the algorithm, the whole live_objects list just exists to keep order, it shouldn't be necessary to maintain a whole copy of
the object just to do that.

It could just be a vector of names (as IDs)?

When kTocNewObjList is set and all the channels are enumerated again, it's NOT required to provide the properties again. So if properties don't change, then simply cloning over the top for last_object will lose the previously accumulated properties

Replaced the look up and update of the object which occurs in the object read code as it was allowing me to update the properties in place.

Moved read_object into TdmsMap implementation so it can access self, not quite sure if this, or passing in the TdmsMap as a parameter is better. Don't actually know if the second is possible when the function is being called from a method on TdmsMap. So will give it a try.

Also, I think I haven't actually set it up to update the all_objects map in place from within the reader, so need to be careful about division of responsibility here.

31/1/2022

Haven't changed the way the object clone behaviour works currently, but I moved read_object into an impl against TdmsMap that gets TdmsMap passed in manually rather than by self bit pointless really but it works and it's a LOT faster. Not sure if that's attributed to the change in algorithm to just do a vec push of a string (as an index into all_objects) rather than cloning objects everywhere, but channel load even for the big file is almost instant, still a fraction slower than Adam's though I think. Will go in and hack out the clones then look to make a commit.

+ Moved read_data_vector to be a pub function inside tdms_datatypes, tidied up call signature and call sites in TdmsFile

Side note added ctrl+alt+i for inlay hint toggle in rust-analyzer

Need to make sure my understanding of how chunk_size behaves/is computed when new objects are added to a segment with existing objects
i.e. only the new objects appear in the meta data but the other objects have raw data.

Confirm it's an ERROR to blindly add to live_objects on the arm where kTocNewObjList has not been set, a channel can appear again purely
to report a change in the number of values, need to figure out how not to update the live_objects map if there's already something in it

+ Fixed error in update rules around the live_object list when kTocNewObjList has not been set, specifically checked if the object is already there i.e. it's not changing order, but some parameter is changing.

Have introduced a crash on the kTocNewObjList arm when unwrapping, not sure how it's possible there's no object in the list.

+ Fixed the crash, but then I think I stopped it loading fast again. I think because it wasn't actually running the indexing calcs because of a previous error.